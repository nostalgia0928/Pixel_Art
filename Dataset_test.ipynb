{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63142022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import tempfile\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms\n",
    "import numpy as np\n",
    "import visdom\n",
    "import scipy\n",
    "import einops\n",
    "import json\n",
    "from geomloss import SamplesLoss\n",
    "from torch import nn, optim\n",
    "from collections import defaultdict\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from math import sqrt\n",
    "from functools import partial, lru_cache\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Parameter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55e20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset defined by my self\n",
    "class WarriorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_names = os.listdir(folder_path)\n",
    "        if transform:\n",
    "            self.transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize((70, 70)),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0, 0, 0), (1, 1, 1))\n",
    "            ])\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_names[index]\n",
    "        image_path = os.path.join(self.folder_path, image_name)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5697bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.getcwd() + \"/Pictures/Warrior\"\n",
    "dataset = WarriorDataset(folder_path, transform=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "shuffle=True, batch_size=16, drop_last=True)\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "\n",
    "print(dataset[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d849ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'width': 32,\n",
    "    'dataset': 'easy_worrior',\n",
    "    'n_channels': 3,\n",
    "    'n_classes': 10,\n",
    "    'batch_size': 16,\n",
    "    'vid_batch': 16,\n",
    "    'latent_dim': 8,  # lower is better modelling but worst interpolation freedom\n",
    "    'lr': 0.005,\n",
    "    'log_every': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cf8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['dataset'] == 'easy_worrior': # Test case\n",
    "    xb = next(train_iterator)\n",
    "    xb = xb.to(device)\n",
    "else:\n",
    "    xb,cb = next(train_iterator)\n",
    "    xb,cb = xb.to(device), cb.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6ebb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nostalgia\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, n_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.LazyConvTranspose2d(512, 4, stride=1, padding=0),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),  # Output: [512, 4, 4]\n",
    "\n",
    "            nn.LazyConvTranspose2d(256, 4, stride=2, padding=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),  # Output: [256, 8, 8]\n",
    "\n",
    "            nn.LazyConvTranspose2d(128, 4, stride=2, padding=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),  # Output: [128, 16, 16]\n",
    "\n",
    "            nn.LazyConvTranspose2d(64, 4, stride=2, padding=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),  # Output: [64, 32, 32]\n",
    "\n",
    "            nn.LazyConvTranspose2d(32, 4, stride=2, padding=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),  # Output: [32, 64, 64]\n",
    "\n",
    "            nn.LazyConvTranspose2d(n_channels, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Output: [3, 128, 128]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.decoder(z)\n",
    "        # Crop from [3, 128, 128] to [3, 69, 44]\n",
    "        x = x[:, :, :69, :44]\n",
    "\n",
    "        # change n_channels above to match number of colours\n",
    "\n",
    "        # 0,0,0,0,1,0,0,0,0,\n",
    "        # softmax(0.3, -0.2, 1.5, ...) -> 0.001, 0.0000, 0.9, 0.00\n",
    "\n",
    "        # # when implementing the softmax, you'll need to remove the sigmoid then do:\n",
    "        # may need to view x to make this work\n",
    "        # x = torch.softmax(x, dim=1)\n",
    "\n",
    "        # to test your softmax code is working, do\n",
    "        # x.sum(dim=1) , check this is all 1's\n",
    "        # inspect say x[0, :, 30, 22] # make sure it looks like a PMF\n",
    "        x = x[:, :, :69, :44]\n",
    "\n",
    "        # Change n_channels above to match the number of colors\n",
    "\n",
    "        # Apply softmax to convert the output to a probability distribution\n",
    "        x = x.view(x.size(0), -1)  # Reshape x to [batch_size, num_features]\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        x = x.view(x.size(0), n_channels, 69, 44)  # Reshape back to [batch_size, n_channels, 69, 44]\n",
    "\n",
    "        # Test the softmax code\n",
    "        print(x.sum(dim=1))  # Check that the sum is 1 for each channel\n",
    "        print(x[0, :, 30, 22])  # Inspect the probability mass function (PMF) for a specific pixel\n",
    "        return x\n",
    "\n",
    "net = Decoder(args['latent_dim'], args['n_channels']).to(device)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=args['lr'])\n",
    "ot_loss_fn = SamplesLoss(\"sinkhorn\", p=2, blur=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178c810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ot_loss(x, y):\n",
    "    return ot_loss_fn(x.view(x.size(0), -1), y.view(y.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c687cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# p(x | z)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# p(x | z, p)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m p_z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 11\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m ot_loss(g, xb)  \u001b[38;5;66;03m# ((g-xb)**2).mean()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, :, :\u001b[38;5;241m69\u001b[39m, :\u001b[38;5;241m44\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Change n_channels above to match the number of colors\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Apply softmax to convert the output to a probability distribution\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reshape x to [batch_size, num_features]\u001b[39;00m\n\u001b[0;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), n_channels, \u001b[38;5;241m69\u001b[39m, \u001b[38;5;241m44\u001b[39m)  \u001b[38;5;66;03m# Reshape back to [batch_size, n_channels, 69, 44]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "logs = {}\n",
    "logs['loss1'] = logs['loss2'] = logs['loss3'] = 0\n",
    "logs['num_stats'] = 0\n",
    "\n",
    "opt.zero_grad()\n",
    "\n",
    "# p(x | z)\n",
    "# p(x | z, p)\n",
    "\n",
    "p_z = torch.randn(args['batch_size'], args['latent_dim'], 1, 1).to(device)\n",
    "g = net(p_z)\n",
    "\n",
    "loss = ot_loss(g, xb)  # ((g-xb)**2).mean()\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.size(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e68921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ce672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e472976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
